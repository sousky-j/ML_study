{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNEjtG6YwBRAfg9BhmtoykV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Corrections"],"metadata":{"id":"vPeQXzSnkR3C"}},{"cell_type":"markdown","source":["fix gemini gems index - original vs translation version"],"metadata":{"id":"4LRB2QgRAIJU"}},{"cell_type":"markdown","source":["#What I learned"],"metadata":{"id":"-mATAxMmkKf0"}},{"cell_type":"markdown","source":["###1. Mahalanobis distance\n","- Euclidean vs Mahalanobis\n","\n","      Euclidean distance assumes equal data distribution.\n","      Mahalanovis distances utilize covariance matrices (in fact, inverse\n","      matrices of covariance matrices) to reflect the structure of the data.\n","      That is, the distance of the data distribution is calculated by reflecting the information of the variance.\n","\n","$$\\Delta^2 = (\\mathbf{y} - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{y} - \\boldsymbol{\\mu}) \\ \\ \\ \\  (\\Delta:Mahalanobis \\ distance)$$\n","$$$$\n","$$\\Delta^2 = \\sum_{j=1}^{d} \\frac{(y_j - \\mu_j)^2}{\\sigma_j^2}$$\n","$$$$\n","$$\\boldsymbol{\\Sigma} = \\mathbf{U} \\boldsymbol{\\Lambda} \\mathbf{U}^T = \\sum_{j=1}^d \\lambda_j \\mathbf{u}_j \\mathbf{u}_j^T \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\boldsymbol{\\Sigma}^{-1} = \\mathbf{U} \\boldsymbol{\\Lambda}^{-1} \\mathbf{U}^T = \\sum_{j=1}^d \\frac{1}{\\lambda_j} \\mathbf{u}_j \\mathbf{u}_j^T$$\n","$$$$\n","$$$$\n","$$z_j = \\mathbf{u}_j^T (\\mathbf{y} - \\boldsymbol{\\mu}) \\ \\ \\ \\ \\ \\ \\ \\ \\boldsymbol{z} = \\boldsymbol{U(y-\\mu)}$$\n","$$$$\n","$$\\Delta^2 = (\\mathbf{y} - \\boldsymbol{\\mu})^T \\left( \\sum_{j=1}^d \\frac{1}{\\lambda_j} \\mathbf{u}_j \\mathbf{u}_j^T \\right) (\\mathbf{y} - \\boldsymbol{\\mu}) = \\sum_{j=1}^d \\frac{z_j^2}{\\lambda_j}$$"],"metadata":{"id":"x6kkVt_UofIo"}}]}